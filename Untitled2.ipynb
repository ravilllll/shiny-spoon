{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b848279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1cc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training and validaion data\n",
    "training = 'training_data'\n",
    "validation = 'validation_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a065364",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean = [0.485,0.465, 0.406], std = [0.229, 0.244, 0.255])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98318cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean = [0.485,0.465, 0.406], std = [0.229, 0.244, 0.255])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8e1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(training, transform = train_transform)\n",
    "val_data = datasets.ImageFolder(validation, transform = val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1fb29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9c33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f712ffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet34(weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe241ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b2ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f94790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(model.fc.in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "585efe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd72fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(num_ftrs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "500c3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteron = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9ddad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optim.SGD([\n",
    "    {'params':model.fc.parameters(),'lr':0.001},\n",
    "    {'params':model.layer4.parameters(),'lr':0.01}\n",
    "], momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b1a273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.StepLR(optimiser, step_size = 7, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62fc48af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.0218\n",
      "Epoch 1/10, Validation Accuracy: 0.9989\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criteron(outputs, labels)\n\u001b[1;32m---> 10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     11\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criteron(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print loss every epoch\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cdc065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'your_trained1_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56d326da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts this image as: dosa\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the path to your image and model\n",
    "image_path = \"image388.png\"  # Change to the path of your new image\n",
    "model_path = 'your_trained1_model.pth'  # Change to the path of your saved model\n",
    "\n",
    "# Define the same transforms that you used for your validation dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define a function for image prediction\n",
    "def predict_image(image_path, model_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Load the trained model\n",
    "    model = models.resnet34(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 3)  # Assuming 3 classes\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "\n",
    "    # Predict the class\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # You can add a class index-to-name mapping based on your dataset\n",
    "    class_names = ['dosa', 'idly', 'vada']  # Replace with your actual class names\n",
    "    predicted_class = class_names[predicted[0]]\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Call the function and print the prediction\n",
    "predicted_class = predict_image(image_path, model_path)\n",
    "print(f\"The model predicts this image as: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8627c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = models.resnet34(weights=None)  # Replace pretrained=False with weights=None\n",
    "num_classes = 3\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('your_trained1_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.feature = None\n",
    "        self.gradient = None\n",
    "        self.register_hooks()\n",
    "\n",
    "    def hook_fn_forward(self, module, input, output):\n",
    "        self.feature = output\n",
    "\n",
    "    def hook_fn_backward(self, module, grad_in, grad_out):\n",
    "        self.gradient = grad_out[0]\n",
    "\n",
    "    def register_hooks(self):\n",
    "        # Use register_full_backward_hook\n",
    "        self.target_layer.register_forward_hook(self.hook_fn_forward)\n",
    "        self.target_layer.register_full_backward_hook(self.hook_fn_backward)\n",
    "\n",
    "    def __call__(self, input_image):\n",
    "        # Forward pass\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_image)\n",
    "\n",
    "        # Backward pass to get the gradient\n",
    "        target = torch.argmax(output)\n",
    "        output[:, target].backward()\n",
    "\n",
    "        # Get the gradient of the target layer\n",
    "        gradient = self.gradient\n",
    "\n",
    "        # Global average pooling on gradients\n",
    "        weights = torch.mean(gradient, dim=(1, 2), keepdim=True)\n",
    "\n",
    "        # Weighted sum of feature maps\n",
    "        cam = torch.sum(weights * self.feature, dim=1)[0]  # Sum over channels and select first sample\n",
    "\n",
    "        # Convert to numpy and apply ReLU\n",
    "        cam = cam.cpu().data.numpy()\n",
    "        cam = np.maximum(cam, 0)\n",
    "\n",
    "        # Normalize the CAM\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "\n",
    "        return cam\n",
    "\n",
    "def get_grad_cam_image(model, image_path, target_layer, save_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_image = preprocess(image).unsqueeze(0)\n",
    "\n",
    "    # Create Grad-CAM instance and generate CAM\n",
    "    grad_cam = GradCAM(model=model, target_layer=target_layer)\n",
    "    cam = grad_cam(input_image)\n",
    "\n",
    "    # Normalize the CAM and convert to 8-bit format\n",
    "    cam = cam - np.min(cam)\n",
    "    cam = cam / np.max(cam)\n",
    "    cam = np.uint8(255 * cam)\n",
    "\n",
    "    # Resize the CAM and convert it to a heatmap\n",
    "    original_image = np.array(image.resize((224, 224)))\n",
    "    cam_resized = cv2.resize(cam, (original_image.shape[1], original_image.shape[0]))\n",
    "    heatmap = cv2.applyColorMap(cam_resized, cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    result = heatmap * 0.4 + original_image * 0.6\n",
    "    result = result.astype('uint8')\n",
    "\n",
    "    # Convert to PIL image and save\n",
    "    result_pil = Image.fromarray(result)\n",
    "    result_pil.save(save_path)\n",
    "\n",
    "    return result_pil\n",
    "\n",
    "# Specify the paths\n",
    "image_path = 'image388.png'\n",
    "save_path = 'grad_cam_result3.png'\n",
    "\n",
    "# Enable gradient tracking\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# Choose a target layer (you can experiment with different layers)\n",
    "target_layer = model.layer4[-1].conv2\n",
    "\n",
    "# Get the Grad-CAM result and save the image\n",
    "result_image = get_grad_cam_image(model, image_path, target_layer, save_path)\n",
    "\n",
    "# Show the image\n",
    "result_image.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19c7fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'grad_cam_images'\n",
    "os.makedirs(folder_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24c451ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to your validation images directory\n",
    "validation_images_directory = 'val_idly'\n",
    "\n",
    "# Get a list of all image files in the directory\n",
    "validation_image_paths = [os.path.join(validation_images_directory, filename) for filename in os.listdir(validation_images_directory) if filename.endswith(('.png', '.jpg'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a7f4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Loop through validation images\n",
    "for image_path in validation_image_paths:\n",
    "    # Generate a unique name for the Grad-CAM result based on the original image name\n",
    "    image_name = os.path.basename(image_path)\n",
    "    grad_cam_save_path = os.path.join(folder_path, f'grad_cam_{image_name}')\n",
    "\n",
    "    # Get the Grad-CAM result and save the image\n",
    "    result_image = get_grad_cam_image(model, image_path, target_layer, grad_cam_save_path)\n",
    "\n",
    "# Optionally, you can display or visualize the results here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d1b3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in validation_image_paths:\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    # ... rest of the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c374ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in validation_image_paths:\n",
    "    if os.path.exists(image_path):\n",
    "        print(f\"Processing image: {image_path}\")\n",
    "        # ... rest of the loop\n",
    "    else:\n",
    "        print(f\"Invalid image path: {image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e790d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
